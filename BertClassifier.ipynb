{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 0. Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nfrom nltk.corpus import stopwords\nimport emoji\nimport torch\nimport re\nfrom transformers import BertForSequenceClassification, BertConfig,BertTokenizer, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\nimport random\nfrom scipy.special import softmax","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. Dataset Load","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\nsubmission_template = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. EDA","metadata":{}},{"cell_type":"code","source":"#train_df['model_a'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df['model_b'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_df.iloc[:,[6,7,8]].sum(axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_df = train_df.iloc[:10]\n# test_df = test_df.iloc[:1000]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Preprocessing","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    #print(\"Beg: \"+text)\n    #1. case folding\n    text = text.lower()\n    #print(\"1: \"+text)\n    #2. remove html tags\n    text = re.sub(r\"<.*?>\", \"\", text)\n    #print(\"2: \"+text)\n    #3. remove URLs\n    text = re.sub(r\"https?:\\/\\/\\S+|www\\.\\S+\", \"\", text)\n    #print(\"3: \"+text)\n    #4. remove punctutation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    #print(\"4: \"+text)\n    #5. remove stopwords\n    stopword_list = stopwords.words('english')\n    text = [word for word in text.split() if word not in stopword_list]\n    text = \" \".join(text)\n    #print(\"5: \"+text)\n    #6. handle emojis\n    text = emoji.demojize(text)\n    #print(\"End: \"+text)\n\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def replace_emptystring(text):\n    if (text == ''):\n        return \"NA\"\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#One hot encoded\n#train_df['label'] = [[train_df['winner_model_a'][i], train_df['winner_model_b'][i], train_df['winner_tie'][i]] for i in range(0, len(train_df))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Class indices\ntrain_df['label'] = [0 if train_df['winner_model_a'][i]==1 else(1 if train_df['winner_model_b'][i]==1 else 2) for i in range(0, len(train_df))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['prompt'] = train_df['prompt'].apply(lambda x: clean_text(x))\ntrain_df['response_a'] = train_df['response_a'].apply(lambda x: clean_text(x))\ntrain_df['response_b'] = train_df['response_b'].apply(lambda x: clean_text(x))\n\ntest_df['prompt'] = test_df['prompt'].apply(lambda x: clean_text(x))\ntest_df['response_a'] = test_df['response_a'].apply(lambda x: clean_text(x))\ntest_df['response_b'] = test_df['response_b'].apply(lambda x: clean_text(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['prompt'] = train_df['prompt'].apply(lambda x: replace_emptystring(x))\ntrain_df['response_a'] = train_df['response_a'].apply(lambda x: replace_emptystring(x))\ntrain_df['response_b'] = train_df['response_b'].apply(lambda x: replace_emptystring(x))\n\ntest_df['prompt'] = test_df['prompt'].apply(lambda x: replace_emptystring(x))\ntest_df['response_a'] = test_df['response_a'].apply(lambda x: replace_emptystring(x))\ntest_df['response_b'] = test_df['response_b'].apply(lambda x: replace_emptystring(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train_df['prompt']=='').value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. LLM Finetune\n#### 4.1 Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.2 Configure maxlength of encoder\nsetting it too high wastes memory, while setting it too low cuts off important context","metadata":{}},{"cell_type":"code","source":"# maxlength_encoded_prompt = 0\n# maxlength_encoded_response1 = 0\n# maxlength_encoded_response2 = 0\n# for prompt, response1, response2 in zip(train_df['prompt'], train_df['response_a'] ,train_df['response_b']):\n#     encoded_prompt = tokenizer.encode(prompt, add_special_tokens=True)\n#     encoded_response1 = tokenizer.encode(response1, add_special_tokens=True)\n#     encoded_response2 = tokenizer.encode(response2, add_special_tokens=True)\n#     maxlength_encoded_prompt = max(maxlength_encoded_prompt, len(encoded_prompt))\n#     maxlength_encoded_response1 = max(maxlength_encoded_response1, len(encoded_response1))\n#     maxlength_encoded_response2 = max(maxlength_encoded_response2, len(encoded_response2))\n\n\n# print(maxlength_encoded_prompt)\n# print(maxlength_encoded_response1)\n# print(maxlength_encoded_response2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Maxlength, avglength comparison: </br>\n4477, 53 - prompt </br>\n8552, 181 - response1 </br>\n9657, 183 - response2 </br>\nSince bert is limited to max #subtokens to be 512, we can: </br>\n1. take front 512 </br>\n2. take middle 512 context </br>\n3. split text into subtexts of 512, classify them and recombine </br>\n4. maybe we can take the median value as the max_length </br>\nhttps://stackoverflow.com/questions/58636587/how-can-i-use-bert-for-long-text-classification\n","metadata":{}},{"cell_type":"code","source":"# avglength_encoded_prompt = 0\n# avglength_encoded_response1 = 0\n# avglength_encoded_response2 = 0\n# for prompt, response1, response2 in zip(train_df['prompt'], train_df['response_a'] ,train_df['response_b']):\n#     encoded_prompt = tokenizer.encode(prompt, add_special_tokens=True)\n#     encoded_response1 = tokenizer.encode(response1, add_special_tokens=True)\n#     encoded_response2 = tokenizer.encode(response2, add_special_tokens=True)\n#     avglength_encoded_prompt += len(encoded_prompt)\n#     avglength_encoded_response1 += len(encoded_response1)\n#     avglength_encoded_response2 += len(encoded_response2)\n\n# print(avglength_encoded_prompt / len(train_df))\n# print(avglength_encoded_response1 / len(train_df))\n# print(avglength_encoded_response2 / len(train_df))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(avglength_encoded_prompt / len(train_df))\n# print(avglength_encoded_response1 / len(train_df))\n# print(avglength_encoded_response2 / len(train_df))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.3 Encoding train dataset","metadata":{}},{"cell_type":"code","source":"input_encodings  = []\nattention_masks = []\n\nfor prompt, response1, response2 in zip(train_df['prompt'], train_df['response_a'], train_df['response_b']):\n    prompt_encoded_dict = tokenizer(\n        prompt,\n        add_special_tokens = False,\n        max_length = 100,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    response1_encoded_dict = tokenizer(\n        response1,\n        add_special_tokens = False,\n        max_length = 204,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    response2_encoded_dict = tokenizer(\n        prompt,\n        add_special_tokens = False,\n        max_length = 204,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    input_id = torch.cat(\n        [torch.tensor([101]), \n         prompt_encoded_dict['input_ids'][0], \n         torch.tensor([102]), \n         response1_encoded_dict['input_ids'][0], \n         torch.tensor([102]),\n         response2_encoded_dict['input_ids'][0],\n         torch.tensor([102])], dim=0)\n    attention_mask = torch.cat(\n        [torch.tensor([1]), \n         prompt_encoded_dict['attention_mask'][0], \n         torch.tensor([1]), \n         response1_encoded_dict['attention_mask'][0], \n         torch.tensor([1]),\n         response2_encoded_dict['attention_mask'][0],\n         torch.tensor([1])], dim=0)\n    input_encodings.append(input_id)\n    attention_masks.append(attention_mask)\n    \ndataset = TensorDataset(\n    torch.stack(input_encodings), \n    torch.stack(attention_masks),\n    torch.tensor(train_df['label']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_input_encodings  = []\ntest_attention_masks = []\n\nfor prompt, response1, response2 in zip(test_df['prompt'], test_df['response_a'], test_df['response_b']):\n    prompt_encoded_dict = tokenizer(\n        prompt,\n        add_special_tokens = False,\n        max_length = 100,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    response1_encoded_dict = tokenizer(\n        response1,\n        add_special_tokens = False,\n        max_length = 204,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    response2_encoded_dict = tokenizer(\n        prompt,\n        add_special_tokens = False,\n        max_length = 204,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    test_input_id = torch.cat(\n        [torch.tensor([101]), \n         prompt_encoded_dict['input_ids'][0], \n         torch.tensor([102]), \n         response1_encoded_dict['input_ids'][0], \n         torch.tensor([102]),\n         response2_encoded_dict['input_ids'][0],\n         torch.tensor([102])], dim=0)\n    test_attention_mask = torch.cat(\n        [torch.tensor([1]), \n         prompt_encoded_dict['attention_mask'][0], \n         torch.tensor([1]), \n         response1_encoded_dict['attention_mask'][0], \n         torch.tensor([1]),\n         response2_encoded_dict['attention_mask'][0],\n         torch.tensor([1])], dim=0)\n    test_input_encodings.append(test_input_id)\n    test_attention_masks.append(test_attention_mask)\n    \ntest_dataset = TensorDataset(\n    torch.stack(test_input_encodings), \n    torch.stack(test_attention_masks),\n    torch.tensor(test_df['id']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.4 Train-Val Split","metadata":{}},{"cell_type":"code","source":"train_dataset, val_dataset = random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.5 Dataloader","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset,\n    sampler = RandomSampler(train_dataset),\n    batch_size = 32\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    sampler = SequentialSampler(val_dataset),\n    batch_size = 32\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    sampler = SequentialSampler(test_dataset),\n    batch_size = 32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.6 Load Bert","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 3,\n    output_attentions = False,\n    output_hidden_states = False\n)\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.6 Finetuning Bert - Training loop","metadata":{}},{"cell_type":"code","source":"seed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_accuracy(predictions, ground_truths):\n    pred = np.argmax(predictions, axis=1)\n    return np.sum(pred==ground_truths) / len(ground_truths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_eval_accuracy = 0.0\nfor epoch in range(0, 2):\n    #train\n    model.train()\n    train_loss = 0.0\n    for (step, batch) in enumerate(train_dataloader):\n        batch_input = batch[0].to(device)\n        batch_attentionmask = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n\n        # print(batch_input.shape)\n        # print(batch_attentionmask.shape)\n        # print(batch_labels.shape)\n\n        optimizer.zero_grad()\n        output = model(batch_input, \n                       token_type_ids = None, \n                       attention_mask = batch_attentionmask, \n                       labels = batch_labels)\n        #print(output.shape)\n        \n        loss = output.loss\n        train_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n    print(f\"Epoch : {epoch} | Training loss : {train_loss / len(train_dataloader)}\")\n\n    #val\n    model.eval()\n    val_loss = 0.0\n    val_accuracy = 0.0\n    for (step, batch) in enumerate(val_dataloader):\n        batch_input = batch[0].to(device)\n        batch_attentionmask = batch[1].to(device)\n        batch_labels = batch[2].to(device)\n        with torch.no_grad():\n            output = model(batch_input, \n                       token_type_ids = None, \n                       attention_mask = batch_attentionmask, \n                       labels = batch_labels)\n        loss = output.loss\n        val_loss += loss.item()\n        logits = output.logits\n        logits = logits.detach().cpu().numpy()\n        ground_truths = batch_labels.to('cpu').numpy()\n        val_accuracy += get_accuracy(logits, ground_truths)\n    print(f\"Val loss : {val_loss / len(val_dataloader)} | Val accuracy : {val_accuracy / len(val_dataloader)}\")\n    if ((val_accuracy / len(val_dataloader)) > best_eval_accuracy):\n        torch.save(model, 'bert-model')\n        best_eval_accuracy = val_accuracy / len(val_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = torch.load('bert-model', weights_only=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.7 Get predictions on test set","metadata":{}},{"cell_type":"code","source":"model.eval()\npredictions = None\nfor (step, batch) in enumerate(test_dataloader):\n    batch_input = batch[0].to(device)\n    batch_attentionmask = batch[1].to(device)\n    batch_id = batch[2].reshape(-1,1)\n    with torch.no_grad():\n        output = model(batch_input, \n                    token_type_ids = None, \n                    attention_mask = batch_attentionmask)\n    logits = output.logits\n    logits = logits.detach().cpu().numpy()\n    pred = torch.tensor(softmax(logits, axis=1))\n    pred = torch.cat([batch_id, pred], dim=1).numpy()\n    if (predictions==None):\n        predictions = pred\n    else:\n        predictions = np.concatenate((predictions, pred), axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id' : predictions[:,0],\n    'winner_model_a' : predictions[:,1],\n    'winner_model_b' : predictions[:,2],\n    'winner_tie' : predictions[:,3]\n})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}