{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Idea:\n#### Create embeddings for Prompt-response1 and Prompt-response2 and then add classification layer on top and train the classification layer specifically","metadata":{}},{"cell_type":"markdown","source":"### 0. Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nfrom nltk.corpus import stopwords\nimport emoji\nimport torch\nimport re\nfrom transformers import BertForSequenceClassification, BertConfig,BertTokenizer, get_linear_schedule_with_warmup, BertModel, AutoTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\nimport random\nimport torch.nn as nn\nfrom scipy.special import softmax","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. Dataset Load","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\nsubmission_template = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/sample_submission.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. EDA","metadata":{}},{"cell_type":"code","source":"#train_df = train_df.iloc[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Preprocessing","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    #print(\"Beg: \"+text)\n    #1. case folding\n    text = text.lower()\n    #print(\"1: \"+text)\n    #2. remove html tags\n    text = re.sub(r\"<.*?>\", \"\", text)\n    #print(\"2: \"+text)\n    #3. remove URLs\n    text = re.sub(r\"https?:\\/\\/\\S+|www\\.\\S+\", \"\", text)\n    #print(\"3: \"+text)\n    #4. remove punctutation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    #print(\"4: \"+text)\n    #5. remove stopwords\n    stopword_list = stopwords.words('english')\n    text = [word for word in text.split() if word not in stopword_list]\n    text = \" \".join(text)\n    #print(\"5: \"+text)\n    #6. handle emojis\n    text = emoji.demojize(text)\n    #print(\"End: \"+text)\n\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def replace_emptystring(text):\n    if (text == ''):\n        return \"NA\"\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#One hot encoded\n#train_df['label'] = [[train_df['winner_model_a'][i], train_df['winner_model_b'][i], train_df['winner_tie'][i]] for i in range(0, len(train_df))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Class indices\ntrain_df['label'] = [0 if train_df['winner_model_a'][i]==1 else(1 if train_df['winner_model_b'][i]==1 else 2) for i in range(0, len(train_df))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['prompt'] = train_df['prompt'].apply(lambda x: clean_text(x))\ntrain_df['response_a'] = train_df['response_a'].apply(lambda x: clean_text(x))\ntrain_df['response_b'] = train_df['response_b'].apply(lambda x: clean_text(x))\n\ntest_df['prompt'] = test_df['prompt'].apply(lambda x: clean_text(x))\ntest_df['response_a'] = test_df['response_a'].apply(lambda x: clean_text(x))\ntest_df['response_b'] = test_df['response_b'].apply(lambda x: clean_text(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['prompt'] = train_df['prompt'].apply(lambda x: replace_emptystring(x))\ntrain_df['response_a'] = train_df['response_a'].apply(lambda x: replace_emptystring(x))\ntrain_df['response_b'] = train_df['response_b'].apply(lambda x: replace_emptystring(x))\n\ntest_df['prompt'] = test_df['prompt'].apply(lambda x: replace_emptystring(x))\ntest_df['response_a'] = test_df['response_a'].apply(lambda x: replace_emptystring(x))\ntest_df['response_b'] = test_df['response_b'].apply(lambda x: replace_emptystring(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. LLM Finetune\n#### 4.1 Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.2 Configure maxlength of encoder\nsetting it too high wastes memory, while setting it too low cuts off important context","metadata":{}},{"cell_type":"markdown","source":"Maxlength, avglength comparison: </br>\n4477, 53 - prompt </br>\n8552, 181 - response1 </br>\n9657, 183 - response2 </br>\nSince bert is limited to max #subtokens to be 512, we can: </br>\n1. take front 512 </br>\n2. take middle 512 context </br>\n3. split text into subtexts of 512, classify them and recombine </br>\n4. maybe we can take the median value as the max_length </br>\nhttps://stackoverflow.com/questions/58636587/how-can-i-use-bert-for-long-text-classification\n","metadata":{}},{"cell_type":"markdown","source":"#### 4.3 Encoding train dataset","metadata":{}},{"cell_type":"code","source":"input_encodings  = [[], []]\nattention_masks = [[], []]\n\nfor prompt, response1, response2 in zip(train_df['prompt'], train_df['response_a'], train_df['response_b']):\n    prompt_response1_encoded_dict = tokenizer(\n        prompt,\n        response1,\n        add_special_tokens = True,\n        max_length = 512,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    prompt_response2_encoded_dict = tokenizer(\n        prompt,\n        response2,\n        add_special_tokens = True,\n        max_length = 512,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n    input_encodings[0].append(prompt_response1_encoded_dict['input_ids'])\n    attention_masks[0].append(prompt_response1_encoded_dict['attention_mask'])\n    input_encodings[1].append(prompt_response2_encoded_dict['input_ids'])\n    attention_masks[1].append(prompt_response2_encoded_dict['attention_mask'])\n    \ndataset = TensorDataset(\n    torch.cat(input_encodings[0], dim=0), \n    torch.cat(attention_masks[0], dim=0),\n    torch.cat(input_encodings[1], dim=0), \n    torch.cat(attention_masks[1], dim=0),\n    torch.tensor(train_df['label']))","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_input_encodings  = [[], []]\ntest_attention_masks = [[], []]\n\nfor prompt, response1, response2 in zip(test_df['prompt'], test_df['response_a'], test_df['response_b']):\n    test_prompt_response1_encoded_dict = tokenizer(\n        prompt,\n        response1,\n        add_special_tokens = True,\n        max_length = 512,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n\n    test_prompt_response2_encoded_dict = tokenizer(\n        prompt,\n        response2,\n        add_special_tokens = True,\n        max_length = 512,\n        padding = 'max_length',\n        return_attention_mask = True,\n        return_tensors = 'pt',\n        truncation=True\n    )\n    test_input_encodings[0].append(test_prompt_response1_encoded_dict['input_ids'])\n    test_attention_masks[0].append(test_prompt_response1_encoded_dict['attention_mask'])\n    test_input_encodings[1].append(test_prompt_response2_encoded_dict['input_ids'])\n    test_attention_masks[1].append(test_prompt_response2_encoded_dict['attention_mask'])\n    \ntest_dataset = TensorDataset(\n    torch.cat(test_input_encodings[0], dim=0), \n    torch.cat(test_attention_masks[0], dim=0),\n    torch.cat(test_input_encodings[1], dim=0), \n    torch.cat(test_attention_masks[1], dim=0),\n    torch.tensor(test_df['id']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.4 Train-Val Split","metadata":{}},{"cell_type":"code","source":"train_dataset, val_dataset = random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.5 Dataloader","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset,\n    sampler = RandomSampler(train_dataset),\n    batch_size = 32\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    sampler = SequentialSampler(val_dataset),\n    batch_size = 32\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    sampler = SequentialSampler(test_dataset),\n    batch_size = 32\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.6 Create bert classifier","metadata":{}},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self, num_labels=3, pretrained_model_name='bert-base-uncased'):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(pretrained_model_name)\n        self.classifier_head = nn.Linear(self.bert.config.hidden_size*2, num_labels)\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n    def forward(self, input1, attentionmask1, input2, attentionmask2):\n        output1 = self.bert(\n            input1, \n            token_type_ids = None, \n            attention_mask = attentionmask1\n        )\n        embedding1 = output1.last_hidden_state[:,0,:]\n        \n        output2 = self.bert(\n            input2, \n            token_type_ids = None, \n            attention_mask = attentionmask2\n        )\n        embedding2 = output2.last_hidden_state[:,0,:]\n\n        final_embedding = torch.cat((embedding1, embedding2), dim=1)\n\n        out = self.classifier_head(final_embedding)\n        #out = self.softmax(out) nn.CrossEntropyLoss() expects logits not probabilities!!!\n        \n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BertClassifier()\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss() # expects logits instead of prediction probabilites & class indices instead of one hot encoded targets!!!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.6 Finetuning Bert - Training loop","metadata":{}},{"cell_type":"code","source":"seed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_accuracy(predictions, ground_truths):\n    pred = np.argmax(predictions, axis=1)\n    return np.sum(pred==ground_truths) / len(ground_truths)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_eval_accuracy = 0.0\nfor epoch in range(0, 2):\n    #train\n    model.train()\n    train_loss = 0.0\n    for (step, batch) in enumerate(train_dataloader):\n        batch_input1 = batch[0].to(device)\n        batch_attentionmask1 = batch[1].to(device)\n        batch_input2 = batch[2].to(device)\n        batch_attentionmask2 = batch[3].to(device)\n        batch_labels = batch[4].to(device)\n\n        optimizer.zero_grad()\n        output = model(batch_input1, \n                       batch_attentionmask1,\n                       batch_input2,\n                       batch_attentionmask2)\n        \n        loss = criterion(output, batch_labels)\n        train_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n    print(f\"Epoch : {epoch} | Training loss : {train_loss / len(train_dataloader)}\")\n\n    #val\n    model.eval()\n    val_loss = 0.0\n    val_accuracy = 0.0\n    for (step, batch) in enumerate(val_dataloader):\n        batch_input1 = batch[0].to(device)\n        batch_attentionmask1 = batch[1].to(device)\n        batch_input2 = batch[2].to(device)\n        batch_attentionmask2 = batch[3].to(device)\n        batch_labels = batch[4].to(device)\n        \n        with torch.no_grad():\n            output = model(batch_input1, \n                       batch_attentionmask1,\n                       batch_input2,\n                       batch_attentionmask2)\n        \n        loss = criterion(output, batch_labels)\n        val_loss += loss.item()\n        logits = output.detach().cpu().numpy()\n        ground_truths = batch_labels.to('cpu').numpy()\n        val_accuracy += get_accuracy(logits, ground_truths)\n    print(f\"Val loss : {val_loss / len(val_dataloader)} | Val accuracy : {val_accuracy / len(val_dataloader)}\")\n    if ((val_accuracy / len(val_dataloader)) > best_eval_accuracy):\n        torch.save(model, 'bert-model')\n        best_eval_accuracy = val_accuracy / len(val_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = torch.load('bert-model', weights_only=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.7 Get predictions on test set","metadata":{}},{"cell_type":"code","source":"model.eval()\npredictions = None\nfor (step, batch) in enumerate(test_dataloader):\n    batch_input1 = batch[0].to(device)\n    batch_attentionmask1 = batch[1].to(device)\n    batch_input2 = batch[2].to(device)\n    batch_attentionmask2 = batch[3].to(device)\n    batch_id = batch[4].reshape(-1,1)\n    \n    with torch.no_grad():\n        output = model(batch_input1, \n                    batch_attentionmask1,\n                    batch_input2,\n                    batch_attentionmask2)\n    logits = output.detach().cpu().numpy()\n    pred = torch.tensor(softmax(logits, axis=1))\n    pred = torch.cat([batch_id, pred], dim=1).numpy()\n    if (predictions==None):\n        predictions = pred\n    else:\n        predictions = np.concatenate((predictions, pred), axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id' : predictions[:,0],\n    'winner_model_a' : predictions[:,1],\n    'winner_model_b' : predictions[:,2],\n    'winner_tie' : predictions[:,3]\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}